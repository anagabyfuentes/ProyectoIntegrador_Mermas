{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unificación de datos históricos\n",
        "\n",
        "Este proceso existe para reunir, limpiar y unificar los registros de merma que hoy se encuentran dispersos en varios archivos y pestañas de Excel. A grandes rasgos, cada bloque cumple una función concreta:\n",
        "\n",
        "Listado de archivos y parámetros básicos\n",
        "Se declara la lista de libros a procesar, los índices de las columnas que interesan, la fila donde empieza el encabezado real y los nombres estándar de las columnas. Con esto se evita escribir la misma lógica una y otra vez y se garantiza que todos los archivos se lean con la misma “plantilla”.\n",
        "\n",
        "Expresión regular sobre el nombre del archivo\n",
        "Antes de abrir cada libro se extraen el año y la procedencia (bodega 321 o isla). Así cada registro queda etiquetado con metadatos coherentes sin depender de lo que haya dentro de las hojas.\n",
        "\n",
        "Bucle de procesamiento\n",
        "Para cada archivo:\n",
        "\n",
        "Se comprueba que exista y que su nombre siga el patrón esperado.\n",
        "\n",
        "Se cargan todas sus pestañas; dentro de cada una se seleccionan únicamente las columnas de interés, se descartan filas vacías y se añaden las columnas Year, Month y Source.\n",
        "\n",
        "Si una hoja no tiene las columnas necesarias o devuelve un error, se salta sin interrumpir el resto del flujo.\n",
        "\n",
        "Limpieza de campos\n",
        "Las columnas numéricas (cajas, kilos, importe) se convierten a número eliminando comas y rellenando nulos con cero; las de texto se sanean para quitar caracteres mal codificados (e, ñ, tildes). Con ello se evitan futuros fallos al sumar o filtrar.\n",
        "\n",
        "Consolidación por libro\n",
        "Las hojas válidas de un mismo archivo se concatenan en un solo DataFrame. Se ordena por mes y producto para que sea fácil de inspeccionar a simple vista.\n",
        "\n",
        "Exportación a CSV\n",
        "El resultado de cada par año–origen se guarda como merma_YYYY_source.csv en UTF-8, formato ligero y multiplataforma que después puede cargarse a una base de datos, un dashboard o un notebook sin pasos extra.\n",
        "\n",
        "El efecto final es pasar de muchos Excel heterogéneos a un conjunto de archivos CSV limpios y homogéneos. Esto simplifica análisis históricos, cálculos de KPIs y cualquier integración posterior con herramientas de business intelligence o scripts de machine learning, eliminando trabajo manual y fuentes de error."
      ],
      "metadata": {
        "id": "HpDdAVMKMVy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj_1-KFlMQbz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "excel_files = [\n",
        "    '/content/2024 merma bod 321.xlsx',\n",
        "    '/content/2024 merma isla.xlsx',\n",
        "    '/content/2023 merma isla.xlsx',\n",
        "    '/content/2023 merma bod 321.xlsx',\n",
        "    '/content/2022 merma isla.xlsx',\n",
        "    '/content/2022 merma bod 321.xlsx'\n",
        "]\n",
        "\n",
        "\n",
        "excel_directory = '.'\n",
        "\n",
        "output_directory = '.'\n",
        "\n",
        "# celdas donde estan las columnas\n",
        "col_indices = [7, 10, 13, 16, 19, 21, 24]\n",
        "col_names = ['Producto', 'Etiqueta', 'Tamaño', 'Color', 'Cajas', 'Kilos', 'Importe_Venta']\n",
        "header_row_index = 11 # Header esta en la 12\n",
        "\n",
        "# los nombres de las ventanas de excel\n",
        "expected_months_sheets = [\n",
        "    \"ENERO\", \"FEBRERO\", \"MARZO\", \"ABRIL\", \"MAYO\", \"JUNIO\",\n",
        "    \"JULIO\", \"AGOSTO\", \"SEPTIEMBRE\", \"OCTUBRE\", \"NOVIEMBRE\", \"DICIEMBRE\"\n",
        "]\n",
        "\n",
        "\n",
        "# buscamos el año y nombre de la bodega (o isla, pero pos si nos mandan otro archivo de otro lugar igual funciona)\n",
        "excel_file_pattern = re.compile(r\"(\\d{4})\\s+merma\\s+(isla|bod\\s*321)\\.xlsx\", re.IGNORECASE)\n",
        "\n",
        "# procesamoe el excel\n",
        "\n",
        "print(\"Starting processing of Excel files...\")\n",
        "output_files_generated = []\n",
        "\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for file_full_path_or_name in excel_files:\n",
        "    file_path = os.path.join(excel_directory, file_full_path_or_name)\n",
        "\n",
        "    excel_file_name = os.path.basename(file_path)\n",
        "\n",
        "    print(f\"\\nProcessing file: {excel_file_name} (from path: {file_path})\")\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"  ERROR: File not found at {file_path}. Skipping.😢\")\n",
        "        continue\n",
        "\n",
        "    match = excel_file_pattern.match(excel_file_name)\n",
        "    if not match:\n",
        "        print(f\"  WARNING: Could not parse filename '{excel_file_name}' for Year/Source. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    year = int(match.group(1))\n",
        "    source_raw = match.group(2)\n",
        "    source = 'bod' if 'bod' in source_raw.lower() else 'isla'\n",
        "\n",
        "    all_sheets_data = []\n",
        "\n",
        "    try:\n",
        "        excel_data = pd.read_excel(file_path, sheet_name=None, header=header_row_index, engine='openpyxl')\n",
        "\n",
        "        # Itera sobre las pestañitas\n",
        "        for sheet_name, df_sheet in excel_data.items():\n",
        "            month = sheet_name.strip().capitalize()\n",
        "            month_upper = sheet_name.strip().upper()\n",
        "\n",
        "            print(f\"  Reading sheet: '{sheet_name}' -> Month: {month}\")\n",
        "\n",
        "            try:\n",
        "                 if len(df_sheet.columns) > max(col_indices):\n",
        "                     df_processed = df_sheet.iloc[:, col_indices].copy()\n",
        "                     df_processed.columns = col_names\n",
        "                 else:\n",
        "                      print(f\"  ERROR: Sheet '{sheet_name}' in {excel_file_name} has fewer columns ({len(df_sheet.columns)}) than expected index ({max(col_indices)}). Skipping sheet.\")\n",
        "                      continue\n",
        "            except IndexError:\n",
        "                 print(f\"  ERROR: Could not select columns by index for sheet '{sheet_name}' in {excel_file_name}. Skipping sheet.\")\n",
        "                 continue\n",
        "            except Exception as e_col:\n",
        "                 print(f\"  ERROR processing columns for sheet '{sheet_name}' in {excel_file_name}: {e_col}. Skipping sheet.\")\n",
        "                 continue\n",
        "\n",
        "            df_processed.dropna(subset=['Producto'], inplace=True)\n",
        "\n",
        "            if not df_processed.empty:\n",
        "                df_processed['Year'] = year\n",
        "                df_processed['Month'] = month\n",
        "                df_processed['Source'] = source\n",
        "                all_sheets_data.append(df_processed)\n",
        "            else:\n",
        "                print(f\"  INFO: No data rows found in sheet '{sheet_name}' after dropping NaNs.\")\n",
        "\n",
        "        # Combina la data anterior\n",
        "        if all_sheets_data:\n",
        "            combined_year_source_df = pd.concat(all_sheets_data, ignore_index=True) #tirar el index\n",
        "            print(f\"  -> Combined {len(all_sheets_data)} sheets for {year}-{source}.\")\n",
        "\n",
        "            numeric_cols = ['Cajas', 'Kilos', 'Importe_Venta']\n",
        "            for col in numeric_cols:\n",
        "                if col in combined_year_source_df.columns:\n",
        "                    if combined_year_source_df[col].dtype == 'object':\n",
        "                       combined_year_source_df[col] = combined_year_source_df[col].astype(str).str.replace(',', '', regex=False)\n",
        "                    combined_year_source_df[col] = pd.to_numeric(combined_year_source_df[col], errors='coerce')\n",
        "                    combined_year_source_df[col] = combined_year_source_df[col].fillna(0)\n",
        "                    if col == 'Cajas':\n",
        "                        combined_year_source_df[col] = combined_year_source_df[col].astype(int)\n",
        "\n",
        "            text_cols = ['Producto', 'Etiqueta', 'Tamaño', 'Color']\n",
        "            try:\n",
        "                for col in text_cols:\n",
        "                    if col in combined_year_source_df.columns:\n",
        "                        if combined_year_source_df[col].dtype == 'object':\n",
        "                            temp_series = combined_year_source_df[col].astype(str)\n",
        "\n",
        "                            temp_series = temp_series.str.replace('Ã±', 'ñ', regex=False).str.replace('Ã‘', 'Ñ', regex=False)\n",
        "                            temp_series = temp_series.str.replace('Ã¡', 'á', regex=False).str.replace('Ã©', 'é', regex=False)\n",
        "\n",
        "                            combined_year_source_df[col] = temp_series\n",
        "            except Exception as e_enc:\n",
        "                print(f\"  Warning: Error during encoding correction for {excel_file_name}: {e_enc}\")\n",
        "\n",
        "            cols_order = ['Year', 'Month', 'Source'] + col_names\n",
        "            final_cols_order = [col for col in cols_order if col in combined_year_source_df.columns]\n",
        "            extra_cols = [col for col in combined_year_source_df.columns if col not in final_cols_order]\n",
        "            combined_year_source_df = combined_year_source_df[final_cols_order + extra_cols]\n",
        "\n",
        "\n",
        "            #guardarlo\n",
        "            output_filename = os.path.join(output_directory, f\"merma_{year}_{source}.csv\")\n",
        "            try:\n",
        "                # Sort antes de guardar para que se vea bonito (no necesary)\n",
        "                month_order_map = {\n",
        "                    'Enero': 1, 'Febrero': 2, 'Marzo': 3, 'Abril': 4, 'Mayo': 5, 'Junio': 6,\n",
        "                    'Julio': 7, 'Agosto': 8, 'Septiembre': 9, 'Octubre': 10, 'Noviembre': 11, 'Diciembre': 12\n",
        "                }\n",
        "\n",
        "                combined_year_source_df['Month_Num'] = combined_year_source_df['Month'].str.capitalize().map(month_order_map)\n",
        "                combined_year_source_df_sorted = combined_year_source_df.sort_values(by=['Month_Num', 'Producto'])\n",
        "                combined_year_source_df_sorted = combined_year_source_df_sorted.drop(columns=['Month_Num'])\n",
        "\n",
        "                combined_year_source_df_sorted.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "                print(f\"  -> Saved combined data to: {output_filename} ({len(combined_year_source_df_sorted)} rows)\")\n",
        "                output_files_generated.append(output_filename)\n",
        "            except Exception as e_save:\n",
        "                print(f\"  ERROR saving file {output_filename}: {e_save}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"  -> No valid data found in any sheet for {excel_file_name}.\")\n",
        "\n",
        "    except Exception as e_excel:\n",
        "        print(f\"  ERROR processing Excel file {excel_file_name}: {e_excel}\")\n",
        "\n",
        "\n",
        "print(f\"\\nProcessing finished. Generated {len(output_files_generated)} CSV files:\")\n",
        "for fname in output_files_generated:\n",
        "    print(f\"- {fname}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Despues de que tenemos los datos unificados por año, los unificamos por bodega"
      ],
      "metadata": {
        "id": "oANioVJ7NNGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "# mejor voy a comentar en español\n",
        "# btw esto es porque estoy en colab\n",
        "data_dir = '/content/'\n",
        "\n",
        "\n",
        "bod_files_to_combine = [\n",
        "    os.path.join(data_dir, 'merma_2024_bod.csv'),\n",
        "    os.path.join(data_dir, 'merma_2023_bod.csv'),\n",
        "    os.path.join(data_dir, 'merma_2022_bod.csv')\n",
        "]\n",
        "\n",
        "\n",
        "list_of_bod_dataframes = []\n",
        "\n",
        "# --- Proceso para archivos 'bod' ---\n",
        "print(\"Procesando archivos 'bod'...\")\n",
        "# itera sobre la lista de archivos, lee cada uno y añade a la lista\n",
        "for file_path in bod_files_to_combine:\n",
        "    try:\n",
        "\n",
        "        df_temp = pd.read_csv(file_path)\n",
        "\n",
        "        list_of_bod_dataframes.append(df_temp)\n",
        "        print(f\"  - Archivo leído correctamente: {file_path} ({len(df_temp)} filas)\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  - Advertencia! Archivo no encontrado: {file_path}. Se omitirá.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error! No se pudo leer el archivo {file_path}: {e}. Se omitirá.\")\n",
        "\n",
        "\n",
        "if list_of_bod_dataframes:\n",
        "    # Concatena todos los df's de la lista en uno solo\n",
        "    # ignore_index=True reinicia el índice del df combinado similar que arriba\n",
        "    df_bod_combinado = pd.concat(list_of_bod_dataframes, ignore_index=True)\n",
        "\n",
        "    print(f\"\\nDataFrame 'bod' combinado creado con éxito. Total de filas: {len(df_bod_combinado)}\")\n",
        "\n",
        "    # Define el nombre del archivo csv de salida para 'bod' (esto hay que cambiarlo para hacerlo mas general y no copiar y pegar como le hice)\n",
        "    output_bod_csv_path = os.path.join(data_dir, 'merma_bod_combinado_2022-2024.csv')\n",
        "\n",
        "    #Guardar el df combinado en un nuevo archivo csv\n",
        "    # index=False evita que pandas escriba el índice del df como una columna en el csv igual que arriba\n",
        "    try:\n",
        "        df_bod_combinado.to_csv(output_bod_csv_path, index=False, encoding='utf-8-sig') # utf-8-sig es bueno para compatibilidad con excel las eñes sobre todo\n",
        "        print(f\"df 'bod' combinado guardado como: {output_bod_csv_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"error! No se pudo guardar el archivo csv combinado para 'bod': {e}\")\n",
        "\n",
        "    # Ahora tenemos el df 'df_bod_combinado'\n",
        "    # las primeras 5 filas\n",
        "    print(\"\\nPrimeras 5 filas del df 'bod' combinado:\")\n",
        "    print(df_bod_combinado.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudo leer ningún archivo 'bod'. No se generó ningún df ni archivo csv combinado.\")\n",
        "\n",
        "# copié y pegué para isla\n",
        "\n",
        "print(\"\\n------------------------------------------\")\n",
        "print(\"Procesando archivos 'isla'...\")\n",
        "\n",
        "isla_files_to_combine = [\n",
        "    os.path.join(data_dir, 'merma_2024_isla.csv'),\n",
        "    os.path.join(data_dir, 'merma_2023_isla.csv'),\n",
        "    os.path.join(data_dir, 'merma_2022_isla.csv')\n",
        "]\n",
        "list_of_isla_dataframes = []\n",
        "\n",
        "for file_path in isla_files_to_combine:\n",
        "    try:\n",
        "        df_temp = pd.read_csv(file_path)\n",
        "        list_of_isla_dataframes.append(df_temp)\n",
        "        print(f\"  - Archivo leído correctamente: {file_path} ({len(df_temp)} filas)\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  - ¡Advertencia! Archivo no encontrado: {file_path}. Se omitirá.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - ¡Error! No se pudo leer el archivo {file_path}: {e}. Se omitirá.\")\n",
        "\n",
        "if list_of_isla_dataframes:\n",
        "    df_isla_combinado = pd.concat(list_of_isla_dataframes, ignore_index=True)\n",
        "    print(f\"\\df 'isla' combinado creado con éxito. Total de filas: {len(df_isla_combinado)}\")\n",
        "    output_isla_csv_path = os.path.join(data_dir, 'merma_isla_combinado_2022-2024.csv')\n",
        "    try:\n",
        "        df_isla_combinado.to_csv(output_isla_csv_path, index=False, encoding='utf-8-sig')\n",
        "        print(f\"df 'isla' combinado guardado como: {output_isla_csv_path}\")\n",
        "        print(\"\\nPrimeras 5 filas del DataFrame 'isla' combinado:\")\n",
        "        print(df_isla_combinado.head())\n",
        "    except Exception as e:\n",
        "        print(f\"error! No se pudo guardar el archivo csv combinado para 'isla': {e}\")\n",
        "else:\n",
        "    print(\"\\nNo se pudo leer ningún archivo 'isla'. No se generó ningún df ni archivo csv combinado.\")"
      ],
      "metadata": {
        "id": "i6Bx4BoCM2ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se genera un DF con las variables exógenas\n",
        "\n",
        "Estos datos se han tomado de fuentes empiricas para la ciudad de Monterrey desde el 2022."
      ],
      "metadata": {
        "id": "0l0WIt_AOVJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "fechas = pd.date_range(\"2022-01-01\", \"2024-12-01\", freq=\"MS\")\n",
        "\n",
        "# Datos numéricos\n",
        "data = {\n",
        "    \"temp_c\": [\n",
        "        13.7, 14.0, 18.0, 23.4, 27.6, 29.8, 30.3, 29.5, 25.7, 20.4, 15.8, 13.5,\n",
        "        14.0, 16.2, 20.5, 25.0, 28.4, 30.1, 30.5, 30.0, 27.0, 22.5, 17.0, 14.2,\n",
        "        14.5, 16.0, 20.0, 25.3, 28.8, 30.5, 30.8, 30.2, 27.5, 22.8, 17.5, 14.0\n",
        "    ],\n",
        "    \"pres\": [\n",
        "        1018, 1016, 1015, 1012, 1008, 1007, 1008, 1008, 1010, 1015, 1020, 1022,\n",
        "        1019, 1016, 1014, 1011, 1008, 1006, 1007, 1008, 1010, 1014, 1018, 1021,\n",
        "        1019, 1016, 1014, 1011, 1008, 1006, 1007, 1008, 1010, 1014, 1018, 1021\n",
        "    ],\n",
        "    \"gas\": [\n",
        "        20.9, 21.2, 21.5, 21.8, 22.0, 22.1, 22.0, 21.9, 21.7, 21.6, 21.4, 21.3,\n",
        "        22.0, 22.3, 22.8, 23.6, 23.0, 23.3, 23.4, 23.3, 23.1, 23.0, 22.8, 22.6,\n",
        "        23.5, 23.7, 23.9, 24.0, 24.1, 24.3, 24.5, 24.4, 24.2, 24.0, 23.8, 23.6\n",
        "    ],\n",
        "    \"humidity\": [\n",
        "        54.6, 57.5, 63.3, 63.1, 67.1, 65.4, 49.7, 63.7, 71.5, 71.6, 71.0, 69.0,\n",
        "        67.5, 65.7, 63.3, 63.0, 67.0, 65.0, 64.2, 63.7, 71.3, 71.6, 71.0, 66.9,\n",
        "        67.5, 65.5, 63.0, 63.0, 67.0, 65.0, 64.0, 64.0, 71.0, 72.0, 71.0, 69.0\n",
        "    ],\n",
        "    \"precip\": [\n",
        "        21.8, 27.3, 29.1, 33.6, 53.2, 62.6, 59.9, 72.7, 199.7, 64.1, 30.0, 22.5,\n",
        "        21.8, 27.3, 29.1, 33.6, 53.2, 62.6, 59.9, 72.7, 199.7, 64.1, 30.0, 22.5,\n",
        "        21.8, 27.3, 29.1, 33.6, 53.2, 62.6, 59.9, 72.7, 199.7, 64.1, 30.0, 22.5\n",
        "    ],\n",
        "    \"cost_mwh\": [\n",
        "        1000, 950, 1100, 1200, 1300, 1400, 1300, 1200, 1100, 1000, 900, 950,\n",
        "        1000, 980, 1100, 1150, 1250, 1300, 1200, 1150, 1100, 1000, 950, 980,\n",
        "        990, 1000, 1100, 1200, 1300, 1350, 1250, 1200, 1150, 1050, 1000, 990\n",
        "    ],\n",
        "    \"price_index\": [\n",
        "        120, 121, 124, 126, 129, 131, 132, 133, 135, 136, 137, 139,\n",
        "        140, 142, 145, 146, 148, 150, 151, 152, 153, 154, 155, 156,\n",
        "        158, 160, 163, 165, 167, 169, 170, 172, 174, 175, 176, 178\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Crear DataFrame con variables exógenas\n",
        "df_monterrey = pd.DataFrame(data, index=fechas)\n",
        "\n",
        "# Mostrar\n",
        "df_monterrey\n"
      ],
      "metadata": {
        "id": "FcMVyGPYOUbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se combinan los 3 DF's así podemos tener un sólo dataframe con las variables exógenas, y los datos de Bodega e Isla"
      ],
      "metadata": {
        "id": "9r8feOyZOtyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datasets de bodega e isla\n",
        "df_bod = pd.read_csv(\"/content/merma_bod_combinado_2022-2024.csv\")\n",
        "df_isla = pd.read_csv(\"/content/merma_isla_combinado_2022-2024.csv\")\n",
        "df_monterrey = pd.read_csv(\"/content/monterrey_2022_2024.csv\")\n",
        "\n",
        "# Añadir columna de origen para trazabilidad\n",
        "df_bod['Origen'] = 'Bodega'\n",
        "df_isla['Origen'] = 'Isla'\n",
        "\n",
        "# Unificar ambos datasets\n",
        "df_merma = pd.concat([df_bod, df_isla], ignore_index=True)\n",
        "\n",
        "meses = {\n",
        "    'enero': 1, 'febrero': 2, 'marzo': 3, 'abril': 4,\n",
        "    'mayo': 5, 'junio': 6, 'julio': 7, 'agosto': 8,\n",
        "    'septiembre': 9, 'octubre': 10, 'noviembre': 11, 'diciembre': 12\n",
        "}\n",
        "\n",
        "df_merma['Month'] = df_merma['Month'].astype(str).str.strip().str.lower()\n",
        "df_merma['Month_Num'] = df_merma['Month'].map(meses)\n",
        "df_merma['Fecha'] = pd.to_datetime(df_merma['Year'].astype(str) + '-' + df_merma['Month_Num'].astype('Int64').astype(str) + '-01', errors='coerce')\n",
        "\n",
        "# Serie temporal combinada de mermas\n",
        "df_merma['Fecha'] = pd.to_datetime(df_merma['Fecha'], utc=True)\n",
        "\n",
        "df_monterrey_reset = df_monterrey.rename(columns={\"Unnamed: 0\": \"Fecha\"})\n",
        "df_monterrey_reset['Fecha'] = pd.to_datetime(df_monterrey_reset['Fecha'], utc=True)\n",
        "\n",
        "df = df_merma.merge(df_monterrey_reset, on=\"Fecha\", how=\"left\")\n",
        "df.drop(labels=[\"Etiqueta\", \"Tamaño\", \"Color\", \"Importe_Venta\", \"Source\", \"Month_Num\"], axis=1, inplace=True)\n",
        "\n",
        "df.to_csv(\"og_exogenas.csv\", index=False)"
      ],
      "metadata": {
        "id": "xncKXRlWO0_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}